{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1><center style=\"color:#2B3698\">Merging the different methods for generating matching keywords</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing the different libraries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stemming.porter2 import stem\n",
    "from itertools import repeat\n",
    "\n",
    "model       = gensim.models.KeyedVectors.load_word2vec_format('/home/theapemachine/data/word2vec/text8-vector.bin', binary=True)\n",
    "nlp         = spacy.load('en')\n",
    "w2v_words   = []\n",
    "wn_words    = []\n",
    "spacy_words = []\n",
    "w2v_stems   = []\n",
    "wn_stems    = []\n",
    "spacy_stems = []\n",
    "words       = []\n",
    "categories  = [\n",
    "    'advice',\n",
    "    'hygiene',\n",
    "    'equipment',\n",
    "    'activities',\n",
    "    'technology',\n",
    "    'info',\n",
    "    'administrative',\n",
    "    'job',\n",
    "    'education',\n",
    "    'home',\n",
    "    'health',\n",
    "    'food'\n",
    "]\n",
    "\n",
    "def most_similar(word):\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:10]\n",
    "\n",
    "def iterate(origin):\n",
    "    try:\n",
    "        for synset in wn.synsets(origin):\n",
    "            for lemma in synset.lemma_names():\n",
    "                if lemma.find('_') == -1:\n",
    "                    w2v_words.append(lemma)\n",
    "                    w2v_stems.append(stem(lemma))\n",
    "\n",
    "        for word in model.most_similar(origin):\n",
    "            if word[0].find('_') == -1:\n",
    "                wn_words.append(word[0])\n",
    "                wn_stems.append(stem(word[0]))\n",
    "\n",
    "        for w in most_similar(nlp.vocab[u''.join([origin])]):\n",
    "            spacy_words.append(w.lower_)\n",
    "            spacy_stems.append(stem(w.lower_))\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def clean(w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words):\n",
    "    for w in w2v_words:\n",
    "        if stem(w) in wn_stems or stem(w) in spacy_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    for w in wn_words:\n",
    "        if stem(w) in w2v_stems or stem(w) in spacy_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    for w in spacy_words:\n",
    "        if stem(w) in w2v_stems or stem(w) in wn_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    return [], [], [], [], [], []\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    iterate(sys.argv[1])\n",
    "\n",
    "    w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "    )\n",
    "\n",
    "    for i in repeat(None, 2):\n",
    "        for w in np.unique(words):\n",
    "            iterate(w)\n",
    "\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "        )\n",
    "\n",
    "    for w in np.unique(words):\n",
    "        print w\n",
    "\n",
    "    words = []\n",
    "else:\n",
    "    for category in categories:\n",
    "        print \"------------------\"\n",
    "        print category\n",
    "        print \"------------------\"\n",
    "        iterate(category)\n",
    "\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "        )\n",
    "\n",
    "        for i in repeat(None, 2):\n",
    "            for w in np.unique(words):\n",
    "                iterate(w)\n",
    "\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "                w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "            )\n",
    "\n",
    "        for w in np.unique(words):\n",
    "            print w\n",
    "\n",
    "        print\n",
    "        words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Merging the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "categories  = [\n",
    "    'advice',\n",
    "    'hygiene',\n",
    "    'equipment',\n",
    "    'activities',\n",
    "    'technology',\n",
    "    'info',\n",
    "    'administrative',\n",
    "    'job',\n",
    "    'education',\n",
    "    'home',\n",
    "    'health',\n",
    "    'food'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hypothesis : our input will be supposed to be a dictionnary of the form {\"name_category\" : {\"matching_keywords\" : {\"Model\" : score}}} and the return would be {\"name_category\" : {\"matching_keywords\" : score}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Question : Deal with stems or all words ?\n",
    "Hypothesis : we will keep only stems in our inputs and then generate the lems afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TO DO : create a function generate_input(models, name_models, category) and return an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_words_wordnet(word) :\n",
    "    name_model = \"wordnet\"\n",
    "    try:\n",
    "        for synset in wn.synsets(origin):\n",
    "            for lemma in synset.lemma_names():\n",
    "                if lemma.find('_') == -1:\n",
    "                    w2v_words.append(lemma)\n",
    "                    w2v_stems.append(stem(lemma))\n",
    "\n",
    "        for word in model.most_similar(origin):\n",
    "            if word[0].find('_') == -1:\n",
    "                wn_words.append(word[0])\n",
    "                wn_stems.append(stem(word[0]))\n",
    "\n",
    "        for w in most_similar(nlp.vocab[u''.join([origin])]):\n",
    "            spacy_words.append(w.lower_)\n",
    "            spacy_stems.append(stem(w.lower_))\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "http://ftp.cs.toronto.edu/pub/gh/Budanitsky+Hirst-2001.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import error : https://stackoverflow.com/questions/15526996/ipython-notebook-locale-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stemming\n",
      "  Downloading stemming-1.0.1.zip\n",
      "Building wheels for collected packages: stemming\n",
      "  Running setup.py bdist_wheel for stemming ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/Abdellah/Library/Caches/pip/wheels/3b/fd/d7/a5c5225045c4856ac54a08feace1a9b262fa385ac0fdfd9155\n",
      "Successfully built stemming\n",
      "Installing collected packages: stemming\n",
      "Successfully installed stemming-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "origin = \"food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('food.n.01'), Synset('food.n.02'), Synset('food.n.03')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food\n",
      "nutrient\n",
      "food\n",
      "solid_food\n",
      "food\n",
      "food_for_thought\n",
      "intellectual_nourish\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets(origin):\n",
    "    for lemma in synset.lemma_names():\n",
    "        print(stem(lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
