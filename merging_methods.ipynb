{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center style=\"color:#2B3698\">Merging the different methods for generating matching keywords</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the different libraries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stemming.porter2 import stem\n",
    "from itertools import repeat\n",
    "\n",
    "model       = gensim.models.KeyedVectors.load_word2vec_format('/home/theapemachine/data/word2vec/text8-vector.bin', binary=True)\n",
    "nlp         = spacy.load('en')\n",
    "w2v_words   = []\n",
    "wn_words    = []\n",
    "spacy_words = []\n",
    "w2v_stems   = []\n",
    "wn_stems    = []\n",
    "spacy_stems = []\n",
    "words       = []\n",
    "categories  = [\n",
    "    'advice',\n",
    "    'hygiene',\n",
    "    'equipment',\n",
    "    'activities',\n",
    "    'technology',\n",
    "    'info',\n",
    "    'administrative',\n",
    "    'job',\n",
    "    'education',\n",
    "    'home',\n",
    "    'health',\n",
    "    'food'\n",
    "]\n",
    "\n",
    "def most_similar(word):\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:10]\n",
    "\n",
    "def iterate(origin):\n",
    "    try:\n",
    "        for synset in wn.synsets(origin):\n",
    "            for lemma in synset.lemma_names():\n",
    "                if lemma.find('_') == -1:\n",
    "                    w2v_words.append(lemma)\n",
    "                    w2v_stems.append(stem(lemma))\n",
    "\n",
    "        for word in model.most_similar(origin):\n",
    "            if word[0].find('_') == -1:\n",
    "                wn_words.append(word[0])\n",
    "                wn_stems.append(stem(word[0]))\n",
    "\n",
    "        for w in most_similar(nlp.vocab[u''.join([origin])]):\n",
    "            spacy_words.append(w.lower_)\n",
    "            spacy_stems.append(stem(w.lower_))\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def clean(w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words):\n",
    "    for w in w2v_words:\n",
    "        if stem(w) in wn_stems or stem(w) in spacy_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    for w in wn_words:\n",
    "        if stem(w) in w2v_stems or stem(w) in spacy_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    for w in spacy_words:\n",
    "        if stem(w) in w2v_stems or stem(w) in wn_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    return [], [], [], [], [], []\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    iterate(sys.argv[1])\n",
    "\n",
    "    w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "    )\n",
    "\n",
    "    for i in repeat(None, 2):\n",
    "        for w in np.unique(words):\n",
    "            iterate(w)\n",
    "\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "        )\n",
    "\n",
    "    for w in np.unique(words):\n",
    "        print w\n",
    "\n",
    "    words = []\n",
    "else:\n",
    "    for category in categories:\n",
    "        print \"------------------\"\n",
    "        print category\n",
    "        print \"------------------\"\n",
    "        iterate(category)\n",
    "\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "        )\n",
    "\n",
    "        for i in repeat(None, 2):\n",
    "            for w in np.unique(words):\n",
    "                iterate(w)\n",
    "\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "                w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "            )\n",
    "\n",
    "        for w in np.unique(words):\n",
    "            print w\n",
    "\n",
    "        print\n",
    "        words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories  = [\n",
    "    'advice',\n",
    "    'hygiene',\n",
    "    'equipment',\n",
    "    'activities',\n",
    "    'technology',\n",
    "    'info',\n",
    "    'administrative',\n",
    "    'job',\n",
    "    'education',\n",
    "    'home',\n",
    "    'health',\n",
    "    'food'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis : our input will be supposed to be a dictionnary of the form {\"name_category\" : {\"matching_keywords\" : {\"Model\" : score}}} and the return would be {\"name_category\" : {\"matching_keywords\" : score}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question : Deal with stems or all words ?\n",
    "Hypothesis : we will keep only stems in our inputs and then generate the lems afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO : create a function generate_input(models, name_models, category) and return an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from stemming.porter2 import stem\n",
    "from nltk.corpus import wordnet_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_wordnet(word, matching_keyword) :\n",
    "    word = wn.synsets(word)[0]\n",
    "    matching_keyword = wn.synsets(matching_keyword)[0]\n",
    "    score = 0\n",
    "    try :\n",
    "        score += word.path_similarity(matching_keyword)\n",
    "        score += word.lch_similarity(matching_keyword)\n",
    "        score += word.wup_similarity(matching_keyword)\n",
    "    except :\n",
    "        pass\n",
    "    # TO DO add an other argument to the function (needs 3 arguments)\n",
    "    # score += word.jcn_similarity(matching_keyword) \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words_wordnet(word, n = 3) :\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    word: word to compute similarities to (can be a category word)\n",
    "    n: number of similar words to get\n",
    "    \"\"\"\n",
    "    \n",
    "    name_model = \"wordnet\"\n",
    "    result = {}\n",
    "    \n",
    "    for synset in wn.synsets(word):\n",
    "        print(word)\n",
    "        for lemma in synset.lemma_names():\n",
    "            result[stem(lemma)] = {name_model : score_wordnet(word, lemma)}\n",
    "    \n",
    "    \n",
    "    for i in range(n) :\n",
    "        for origin in result.keys():\n",
    "            print(origin)\n",
    "            for synset in wn.synsets(origin):\n",
    "                for lemma in synset.lemma_names():\n",
    "                    result[stem(lemma)] = {name_model : score_wordnet(word, lemma)}\n",
    "        \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food\n",
      "food\n",
      "food\n",
      "food\n",
      "nutrient\n",
      "food_for_thought\n",
      "solid_food\n",
      "intellectual_nourish\n",
      "nourish\n",
      "nutrit\n",
      "aliment\n",
      "food\n",
      "solid_food\n",
      "alimentari\n",
      "nutriti\n",
      "intellectual_nourish\n",
      "nutrient\n",
      "food_for_thought\n",
      "nourish\n",
      "nutrit\n",
      "aliment\n",
      "food\n",
      "susten\n",
      "nutriment\n",
      "solid_food\n",
      "alimentari\n",
      "nutriti\n",
      "victual\n",
      "sustain\n",
      "nurtur\n",
      "intellectual_nourish\n",
      "nutrient\n",
      "food_for_thought\n",
      "nutrifi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'affirm': {'wordnet': 0},\n",
       " u'aliment': {'wordnet': 0},\n",
       " u'alimentari': {'wordnet': 0},\n",
       " u'comest': {'wordnet': 4.353529888257349},\n",
       " u'confirm': {'wordnet': 0},\n",
       " u'corrobor': {'wordnet': 0},\n",
       " u'eatabl': {'wordnet': 4.353529888257349},\n",
       " u'edibl': {'wordnet': 4.353529888257349},\n",
       " u'food': {'wordnet': 5.637586159726386},\n",
       " u'food_for_thought': {'wordnet': 1.6168192485505222},\n",
       " u'get': {'wordnet': 0.9743074561930456},\n",
       " u'have': {'wordnet': 2.2345331535282154},\n",
       " u'hold': {'wordnet': 1.2924170220450686},\n",
       " u'hold_up': {'wordnet': 0},\n",
       " u'intellectual_nourish': {'wordnet': 1.6168192485505222},\n",
       " u'keep': {'wordnet': 1.4972666445037728},\n",
       " u'keep_up': {'wordnet': 0},\n",
       " u'maintain': {'wordnet': 0},\n",
       " u'nourish': {'wordnet': 0},\n",
       " u'nurtur': {'wordnet': 1.6168192485505222},\n",
       " u'nutrient': {'wordnet': 5.637586159726386},\n",
       " u'nutrifi': {'wordnet': 0},\n",
       " u'nutriment': {'wordnet': 4.353529888257349},\n",
       " u'nutrit': {'wordnet': 0},\n",
       " u'nutriti': {'wordnet': 0},\n",
       " u'pabulum': {'wordnet': 4.353529888257349},\n",
       " u'prolong': {'wordnet': 0},\n",
       " u'solid_food': {'wordnet': 2.828148247292286},\n",
       " u'substanti': {'wordnet': 0},\n",
       " u'suffer': {'wordnet': 0},\n",
       " u'support': {'wordnet': 1.4972666445037728},\n",
       " u'sustain': {'wordnet': 0},\n",
       " u'susten': {'wordnet': 4.353529888257349},\n",
       " u'victual': {'wordnet': 4.353529888257349}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_words_wordnet(\"food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar_word2vec(word, n, path, model_name = \"text8-vector.bin\"):\n",
    "    \"\"\" Most similar words to word and their scores.\n",
    "    Parameters\n",
    "    ----------\n",
    "    word: word to compute similarities to (can be a category word)\n",
    "    n: number of similar words to get\n",
    "    path: path of the pretrained word2vec model\n",
    "    model_name: name of the pretrained word2vec model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    d: {word: {similar_word: {word2vec: word2vec_score}}\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    # Load Google's pre-trained Word2Vec model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(path, \"text8-vector.bin\"), binary=True) \n",
    "    d[word] = {key: {\"word2vec\": value} for (key, value) in model.most_similar(word, topn = n)}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food': {'bananas': {'word2vec': 0.6068102717399597},\n",
       "  'beef': {'word2vec': 0.6227272748947144},\n",
       "  'beverages': {'word2vec': 0.6333991289138794},\n",
       "  'canned': {'word2vec': 0.6140711903572083},\n",
       "  'cashew': {'word2vec': 0.6168662905693054},\n",
       "  'chemicals': {'word2vec': 0.5928491950035095},\n",
       "  'citrus': {'word2vec': 0.5871732831001282},\n",
       "  'cocoa': {'word2vec': 0.6041184663772583},\n",
       "  'cooking': {'word2vec': 0.6009650826454163},\n",
       "  'dairy': {'word2vec': 0.6460014581680298},\n",
       "  'fermented': {'word2vec': 0.5760336518287659},\n",
       "  'fertilizers': {'word2vec': 0.5925315618515015},\n",
       "  'fish': {'word2vec': 0.5927667617797852},\n",
       "  'foods': {'word2vec': 0.7001853585243225},\n",
       "  'foodstuffs': {'word2vec': 0.6066914200782776},\n",
       "  'fruit': {'word2vec': 0.5887770652770996},\n",
       "  'fruits': {'word2vec': 0.6141229271888733},\n",
       "  'grain': {'word2vec': 0.5781475305557251},\n",
       "  'ingredients': {'word2vec': 0.5744912624359131},\n",
       "  'liquor': {'word2vec': 0.5889161825180054},\n",
       "  'livestock': {'word2vec': 0.6447887420654297},\n",
       "  'maize': {'word2vec': 0.6075355410575867},\n",
       "  'meat': {'word2vec': 0.6837040185928345},\n",
       "  'milk': {'word2vec': 0.5991055965423584},\n",
       "  'nutrition': {'word2vec': 0.5815523862838745},\n",
       "  'pork': {'word2vec': 0.5774648189544678},\n",
       "  'potatoes': {'word2vec': 0.5982800722122192},\n",
       "  'poultry': {'word2vec': 0.629684329032898},\n",
       "  'products': {'word2vec': 0.6491330862045288},\n",
       "  'rice': {'word2vec': 0.600838303565979},\n",
       "  'seafood': {'word2vec': 0.6074264049530029},\n",
       "  'shrimp': {'word2vec': 0.5876439809799194},\n",
       "  'sorghum': {'word2vec': 0.5954698324203491},\n",
       "  'soybeans': {'word2vec': 0.5745106339454651},\n",
       "  'sugar': {'word2vec': 0.5735200643539429},\n",
       "  'sugarcane': {'word2vec': 0.5917256474494934},\n",
       "  'textiles': {'word2vec': 0.6155574321746826},\n",
       "  'tobacco': {'word2vec': 0.6639424562454224},\n",
       "  'vegetable': {'word2vec': 0.6230506300926208},\n",
       "  'vegetables': {'word2vec': 0.6334857940673828}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_word2vec(\"food\", 40, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ftp.cs.toronto.edu/pub/gh/Budanitsky+Hirst-2001.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import error : https://stackoverflow.com/questions/15526996/ipython-notebook-locale-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = \"food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for synset in wn.synsets(origin):\n",
    "    for lemma in synset.lemma_names():\n",
    "        l.append(stem(lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for origin in l :\n",
    "    for synset in wn.synsets(origin):\n",
    "        for lemma in synset.lemma_names():\n",
    "            l.append(stem(lemma))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
