{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center style=\"color:#2B3698\">Merging the different methods for generating matching keywords</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents :\n",
    "1. [Synset selection for each category](#syn)\n",
    "2. [Wordnet Scores](#score-wordnet)\n",
    "3. [Vocabulary Generator](#vocab)\n",
    "4. [Putting All Together](#all)\n",
    "5. [Next Steps](#next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the different libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Import libraries \n",
    "from nltk.corpus import wordnet as wn\n",
    "from stemming.porter2 import stem\n",
    "from nltk.corpus import wordnet_ic\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Vocabulary creation* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process that we will use in order to generate the different words will be the following :\n",
    "<ol> \n",
    "<li> Define different models that can generate n words with a score (wordnet, word2vec).\n",
    "   <br>The output of this generators will be of the form {word : {\"model\" : score}}.\n",
    "<li> Given the outputs of different models, merge them into one dictionnary and define \n",
    "   the weights for each model.\n",
    "<li> Loop through all the categories and generate all the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining global variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path of the pretrained word2vec \n",
    "PATH = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\"\n",
    "\n",
    "# Categories used in weanswer\n",
    "CATEGORIES = [\n",
    "    'advice',\n",
    "    'hygiene',\n",
    "    'equipment',\n",
    "    'activities',\n",
    "    'technology',\n",
    "    'info',\n",
    "    'administrative',\n",
    "    'job',\n",
    "    'education',\n",
    "    'home',\n",
    "    'health',\n",
    "    'food'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synset selection for each category <a id = \"syn\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to choose the right synset for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> advice\n",
      "0 - ['advice'] : a proposal for an appropriate course of action\n",
      "\n",
      "\n",
      ">>> hygiene\n",
      "0 - ['hygiene'] : a condition promoting sanitary practices\n",
      "1 - ['hygiene', 'hygienics'] : the science concerned with the prevention of illness and maintenance of health\n",
      "\n",
      "\n",
      ">>> equipment\n",
      "0 - ['equipment'] : an instrumentality needed for an undertaking or to perform a service\n",
      "\n",
      "\n",
      ">>> activities\n",
      "0 - ['activity'] : any specific behavior\n",
      "1 - ['action', 'activity', 'activeness'] : the state of being active\n",
      "2 - ['bodily_process', 'body_process', 'bodily_function', 'activity'] : an organic process that takes place in the body\n",
      "3 - ['activity'] : (chemistry) the capacity of a substance to take part in a chemical reaction\n",
      "4 - ['natural_process', 'natural_action', 'action', 'activity'] : a process existing in or produced by nature (rather than by the intent of human beings)\n",
      "5 - ['activeness', 'activity'] : the trait of being active; moving or acting rapidly and energetically\n",
      "\n",
      "\n",
      ">>> technology\n",
      "0 - ['technology', 'engineering'] : the practical application of science to commerce or industry\n",
      "1 - ['engineering', 'engineering_science', 'applied_science', 'technology'] : the discipline dealing with the art or science of applying scientific knowledge to practical problems\n",
      "\n",
      "\n",
      ">>> info\n",
      "0 - ['information', 'info'] : a message received and understood\n",
      "\n",
      "\n",
      ">>> administrative\n",
      "0 - ['administrative'] : of or relating to or responsible for administration\n",
      "\n",
      "\n",
      ">>> job\n",
      "0 - ['occupation', 'business', 'job', 'line_of_work', 'line'] : the principal activity in your life that you do to earn money\n",
      "1 - ['job', 'task', 'chore'] : a specific piece of work required to be done as a duty or for a specific fee\n",
      "2 - ['job'] : a workplace; as in the expression \"on the job\"; \n",
      "3 - ['job'] : an object worked on; a result produced by working\n",
      "4 - ['job'] : the responsibility to do something\n",
      "5 - ['job'] : the performance of a piece of work\n",
      "6 - ['job'] : a damaging piece of work\n",
      "7 - ['problem', 'job'] : a state of difficulty that needs to be resolved\n",
      "8 - ['Job'] : a Jewish hero in the Old Testament who maintained his faith in God in spite of afflictions that tested him\n",
      "9 - ['Job'] : any long-suffering person who withstands affliction without despairing\n",
      "10 - ['job'] : (computer science) a program application that may consist of several steps but is a single logical unit\n",
      "11 - ['Job', 'Book_of_Job'] : a book in the Old Testament containing Job's pleas to God about his afflictions and God's reply\n",
      "12 - ['caper', 'job'] : a crime (especially a robbery)\n",
      "13 - ['job'] : profit privately from public office and official business\n",
      "14 - ['subcontract', 'farm_out', 'job'] : arranged for contracted work to be done by others\n",
      "15 - ['job'] : work occasionally\n",
      "16 - ['speculate', 'job'] : invest at a risk\n",
      "\n",
      "\n",
      ">>> education\n",
      "0 - ['education', 'instruction', 'teaching', 'pedagogy', 'didactics', 'educational_activity'] : the activities of educating or instructing; activities that impart knowledge or skill\n",
      "1 - ['education'] : knowledge acquired by learning and instruction\n",
      "2 - ['education'] : the gradual process of acquiring knowledge\n",
      "3 - ['education'] : the profession of teaching (especially at a school or college or university)\n",
      "4 - ['education', 'training', 'breeding'] : the result of good upbringing (especially knowledge of correct social behavior)\n",
      "5 - ['Department_of_Education', 'Education_Department', 'Education'] : the United States federal department that administers all federal programs dealing with education (including federal aid to educational institutions and students); created 1979\n",
      "\n",
      "\n",
      ">>> home\n",
      "0 - ['home', 'place'] : where you live at a particular time\n",
      "1 - ['dwelling', 'home', 'domicile', 'abode', 'habitation', 'dwelling_house'] : housing that someone is living in\n",
      "2 - ['home'] : the country or state or city where you live\n",
      "3 - ['home_plate', 'home_base', 'home', 'plate'] : (baseball) base consisting of a rubber slab where the batter stands; it must be touched by a base runner in order to score\n",
      "4 - ['base', 'home'] : the place where you are stationed and from which missions start and end\n",
      "5 - ['home'] : place where something began and flourished\n",
      "6 - ['home'] : an environment offering affection and security\n",
      "7 - ['family', 'household', 'house', 'home', 'menage'] : a social unit living together\n",
      "8 - ['home', 'nursing_home', 'rest_home'] : an institution where people are cared for\n",
      "9 - ['home'] : provide with, or send to, a home\n",
      "10 - ['home'] : return home accurately from a long distance\n",
      "11 - ['home'] : used of your own ground\n",
      "12 - ['home'] : relating to or being where one lives or where one's roots are\n",
      "13 - ['home', 'interior', 'internal', 'national'] : inside the country\n",
      "14 - ['home'] : at or to or in the direction of one's home or family\n",
      "15 - ['home'] : on or to the point aimed at\n",
      "16 - ['home'] : to the fullest extent; to the heart\n",
      "\n",
      "\n",
      ">>> health\n",
      "0 - ['health', 'wellness'] : a healthy state of wellbeing free from disease\n",
      "1 - ['health'] : the general condition of body and mind\n",
      "\n",
      "\n",
      ">>> food\n",
      "0 - ['food', 'nutrient'] : any substance that can be metabolized by an animal to give energy and build tissue\n",
      "1 - ['food', 'solid_food'] : any solid substance (as opposed to liquid) that is used as a source of nourishment\n",
      "2 - ['food', 'food_for_thought', 'intellectual_nourishment'] : anything that provides mental stimulus for thinking\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each category, for each synset print its lemma names and the definition\n",
    "for category in CATEGORIES:\n",
    "    print(\">>> {}\".format(category))\n",
    "    i = 0\n",
    "    for synset in wn.synsets(category):\n",
    "        print(i, \"-\", synset.lemma_names(), \":\", synset.definition())\n",
    "        i += 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each category, we will select only the relevant synsets (the numbers represent the indices of the right synsets)\n",
    "d_category_synsets = {\"advice\": [0], \"hygiene\": [0], \"equipment\": [0], \"activities\": [0], \"technology\": [0], \"info\": [0], \n",
    "                      \"administrative\": [0], \"job\": [0, 1], \"education\": [0], \"home\": [0], \"health\": [0], \"food\": [0,1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wordnet scores   <a id=\"score-wordnet\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute 4 different scores for wordnet (http://www.nltk.org/howto/wordnet.html):\n",
    "<li> path_similarity score :  Return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1.\n",
    "<li> lch_similarity : Return a score denoting how similar two word senses are, based on the shortest path that connects the senses (as above) and the maximum depth of the taxonomy in which the senses occur. The relationship is given as -log(p/2d) where p is the shortest path length and d the taxonomy depth. (Here is a website that simulates the path to better understand the taxonomy : http://ws4jdemo.appspot.com/?mode=w&s1=&w1=cat&s2=&w2=dog.\n",
    "<li> wup_similarity : The Wu & Palmer measure (wup) calculates similarity by considering the depths of the two concepts in wordnet, along with the depth of the LCS (least common ancestor in the taxonomy) The formula is score = 2*depth(lcs) / (depth(s1) + depth(s2)). This means that 0 $<$ score $<=$ 1. The score can never be zero because the depth of the LCS is never zero (the depth of the root of a taxonomy is one). The score is one if the two input concepts are the same.\n",
    "<li> jcn_similarity : Jiang-Conrath Similarity Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. The relationship is given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs)). (Information Content (IC) is a measure of specificity for a concept. Higher values are associated with more specific concepts (e.g., pitch fork), while those with lower values are more general (e.g., idea). In- formation Content is computed based on frequency counts of concepts as found in a corpus of text).\n",
    "\n",
    "Also, this article http://ftp.cs.toronto.edu/pub/gh/Budanitsky+Hirst-2001.pdf presents the different score computation and compares them to human judgment of semantic similarity. The similarity that matched the most the human judgment was jcn_similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_wordnet(word, matching_keyword) : \n",
    "    \"\"\"\n",
    "    Function that generates a score for the wordnet outputs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word              : string we want to compute the similarity to.\n",
    "    matching_keywords : string for computring the similarity.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score : The similarity score (float).\n",
    "    \"\"\"\n",
    "    \n",
    "    # We will nomalise the scores for each similarity\n",
    "    min_max_values = {'path' : [0, 1], 'lch' : [0, 3.6375861597263857 ], 'wup' : [0,1], 'jcn' : [0, 10000]}\n",
    "    # Reference for range : (lch) https://stackoverflow.com/questions/20112828/maximum-score-in-wordnet-based-similarity\n",
    "    # (jcn) https://stackoverflow.com/questions/35751207/how-to-normalize-similarity-measurements-lch-wup-path-res-lin-jcn-between\n",
    "    \n",
    "    # For jcn we will take 10 000 as the maximum (quite arbitrary but seemed relevant)\n",
    "    \n",
    "    word = wn.synsets(word)[0]\n",
    "    matching_keyword = wn.synsets(matching_keyword)[0]\n",
    "    score = 0\n",
    "    try :\n",
    "        score += word.path_similarity(matching_keyword)\n",
    "    except :\n",
    "        pass\n",
    "    try : \n",
    "        score += (word.lch_similarity(matching_keyword) /  min_max_values['lch'][1])\n",
    "    except :\n",
    "        pass\n",
    "    try :\n",
    "        score += word.wup_similarity(matching_keyword)\n",
    "    except :\n",
    "        pass\n",
    "    try : \n",
    "        brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "        score += (word.jcn_similarity(matching_keyword) / min_max_values['jcn'][1])\n",
    "    except :\n",
    "        pass\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary generator  <a id=\"vocab\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a class that is able to generate different words according to different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep only stems in our inputs and then generate the lems afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabularyGenerator():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def generate_words_wordnet(self, word, n, depth = 4, synsets_indices = d_category_synsets) :\n",
    "        \"\"\"\n",
    "        Function that generates matching keywords given a word.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word    : word to compute similarities to (can be a category word).\n",
    "        depth   : number of layers we use when generating maching keywords.\n",
    "        n       : number of words we take.\n",
    "        synsets : dictionary mapping a word to the a list of indices\n",
    "                 specifying its relevant synsets in wordnet\n",
    "                 Ex: {\"food\": [0]}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        d : {similar_word: {wordnet: wordnet_score}\n",
    "        \"\"\"\n",
    "\n",
    "        name_model = \"wordnet\"\n",
    "        d = {}\n",
    "\n",
    "        # First iteration (select only relevant synsets as specified in d_category_synsets)\n",
    "        # All the synsets of word\n",
    "        synsets = wn.synsets(word)    \n",
    "        # Only relevant synsets of word\n",
    "        if word in CATEGORIES:\n",
    "            synsets = [synsets[i] for i in range(len(synsets)) if i in synsets_indices[word]]\n",
    "        for synset in tqdm(synsets):\n",
    "            for lemma in synset.lemma_names():\n",
    "                d[stem(lemma)] = {name_model : score_wordnet(word, lemma)}\n",
    "\n",
    "        # Other iterations\n",
    "\n",
    "        for i in tqdm(range(depth)) :\n",
    "            dic = d.copy()\n",
    "            for origin in dic.keys():\n",
    "                for synset in wn.synsets(origin):\n",
    "                    for lemma in synset.lemma_names():\n",
    "                        d[stem(lemma)] = {name_model : score_wordnet(word, lemma)}\n",
    "\n",
    "        if n > len(d) :\n",
    "            return d\n",
    "\n",
    "        d_max_values = {k: d[k] for k in sorted(d, key = lambda k: d[k][\"wordnet\"], reverse = True)[:n]}\n",
    "\n",
    "        return d_max_values\n",
    "\n",
    "    def generate_words_word2vec(self, word, n, path = PATH, model_name = \"text8-vector.bin\"):\n",
    "        \"\"\" \n",
    "        Most similar words to word and their scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word       : word to compute similarities to (can be a category word)\n",
    "        n          : number of similar words to get\n",
    "        path       : path of the pretrained word2vec model\n",
    "        model_name : name of the pretrained word2vec model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        d : {similar_word: {word2vec: word2vec_score}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load Google's pre-trained Word2Vec model\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(path, \"text8-vector.bin\"), binary=True) \n",
    "        d = {key: {\"word2vec\": value} for (key, value) in model.most_similar(word, topn = n)}\n",
    "        return d\n",
    "    \n",
    "    def generate_word_spacy(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  8.98s/it]\n",
      "100%|██████████| 4/4 [05:32<00:00, 94.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "generator = VocabularyGenerator()\n",
    "d_wordnet = generator.generate_words_wordnet(\"food\", 40)\n",
    "d_word2vec = generator.generate_words_word2vec(\"food\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aliment': {'wordnet': 2.218539496664136},\n",
       " 'back': {'wordnet': 0.9169821263560435},\n",
       " 'backup': {'wordnet': 0.9169821263560435},\n",
       " 'baffl': {'wordnet': 0.7746942172313404},\n",
       " 'beat': {'wordnet': 0.8404107581326701},\n",
       " 'begin': {'wordnet': 0.7746942172313404},\n",
       " 'brook': {'wordnet': 0.9169821263560435},\n",
       " 'cargo_deck': {'wordnet': 0.7174237954982008},\n",
       " 'cargo_hold': {'wordnet': 0.7174237954982008},\n",
       " 'come': {'wordnet': 1.107911697275812},\n",
       " 'comest': {'wordnet': 2.218539496664136},\n",
       " 'conserv': {'wordnet': 1.3407650777506728},\n",
       " 'curb': {'wordnet': 0.7174237954982008},\n",
       " 'donjon': {'wordnet': 0.7174237954982008},\n",
       " 'draw': {'wordnet': 0.7746942172313404},\n",
       " 'dungeon': {'wordnet': 0.7174237954982008},\n",
       " 'eatabl': {'wordnet': 2.218539496664136},\n",
       " 'edibl': {'wordnet': 2.218539496664136},\n",
       " 'food': {'wordnet': 3.0},\n",
       " 'gravel': {'wordnet': 1.107911697275812},\n",
       " 'guard': {'wordnet': 0.8404107581326701},\n",
       " 'handgrip': {'wordnet': 0.9169821263560435},\n",
       " 'handl': {'wordnet': 0.9169821263560435},\n",
       " 'harbor': {'wordnet': 0.7746942172313404},\n",
       " 'harbour': {'wordnet': 0.7746942172313404},\n",
       " 'have': {'wordnet': 1.0079116972758122},\n",
       " 'moder': {'wordnet': 0.9169821263560435},\n",
       " 'nurs': {'wordnet': 0.7746942172313404},\n",
       " 'nutrient': {'wordnet': 3.0},\n",
       " 'nutriment': {'wordnet': 2.218539496664136},\n",
       " 'nutrit': {'wordnet': 1.0079116972758122},\n",
       " 'pabulum': {'wordnet': 2.218539496664136},\n",
       " 'produc': {'wordnet': 1.2195529565385517},\n",
       " 'rich_person': {'wordnet': 1.0079116972758122},\n",
       " 'solid_food': {'wordnet': 1.3575533219658062},\n",
       " 'stick': {'wordnet': 0.7746942172313404},\n",
       " 'stomach': {'wordnet': 0.7746942172313404},\n",
       " 'susten': {'wordnet': 2.218539496664136},\n",
       " 'victual': {'wordnet': 2.218539496664136},\n",
       " 'wealthy_person': {'wordnet': 1.0079116972758122}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bananas': {'word2vec': 0.6068102717399597},\n",
       " 'beef': {'word2vec': 0.6227272748947144},\n",
       " 'beverages': {'word2vec': 0.6333991289138794},\n",
       " 'canned': {'word2vec': 0.6140711903572083},\n",
       " 'cashew': {'word2vec': 0.6168662905693054},\n",
       " 'chemicals': {'word2vec': 0.5928491950035095},\n",
       " 'citrus': {'word2vec': 0.5871732831001282},\n",
       " 'cocoa': {'word2vec': 0.6041184663772583},\n",
       " 'cooking': {'word2vec': 0.6009650826454163},\n",
       " 'dairy': {'word2vec': 0.6460014581680298},\n",
       " 'fermented': {'word2vec': 0.5760336518287659},\n",
       " 'fertilizers': {'word2vec': 0.5925315618515015},\n",
       " 'fish': {'word2vec': 0.5927667617797852},\n",
       " 'foods': {'word2vec': 0.7001853585243225},\n",
       " 'foodstuffs': {'word2vec': 0.6066914200782776},\n",
       " 'fruit': {'word2vec': 0.5887770652770996},\n",
       " 'fruits': {'word2vec': 0.6141229271888733},\n",
       " 'grain': {'word2vec': 0.5781475305557251},\n",
       " 'ingredients': {'word2vec': 0.5744912624359131},\n",
       " 'liquor': {'word2vec': 0.5889161825180054},\n",
       " 'livestock': {'word2vec': 0.6447887420654297},\n",
       " 'maize': {'word2vec': 0.6075355410575867},\n",
       " 'meat': {'word2vec': 0.6837040185928345},\n",
       " 'milk': {'word2vec': 0.5991055965423584},\n",
       " 'nutrition': {'word2vec': 0.5815523862838745},\n",
       " 'pork': {'word2vec': 0.5774648189544678},\n",
       " 'potatoes': {'word2vec': 0.5982800722122192},\n",
       " 'poultry': {'word2vec': 0.629684329032898},\n",
       " 'products': {'word2vec': 0.6491330862045288},\n",
       " 'rice': {'word2vec': 0.600838303565979},\n",
       " 'seafood': {'word2vec': 0.6074264049530029},\n",
       " 'shrimp': {'word2vec': 0.5876439809799194},\n",
       " 'sorghum': {'word2vec': 0.5954698324203491},\n",
       " 'soybeans': {'word2vec': 0.5745106339454651},\n",
       " 'sugar': {'word2vec': 0.5735200643539429},\n",
       " 'sugarcane': {'word2vec': 0.5917256474494934},\n",
       " 'textiles': {'word2vec': 0.6155574321746826},\n",
       " 'tobacco': {'word2vec': 0.6639424562454224},\n",
       " 'vegetable': {'word2vec': 0.6230506300926208},\n",
       " 'vegetables': {'word2vec': 0.6334857940673828}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Putting it all together <a id=\"all\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a function that normalizes the scores of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_scores(d_scores):\n",
    "    \"\"\"\"\n",
    "    The function normalize the scores (between 0 and 1).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_scores : dictionnary of the form {matching_keywords : {model : score}}.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d_normalized : same dictionnary with normalized scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the model name\n",
    "    modelname = list(list(d_scores.values())[0].keys())[0]\n",
    "        \n",
    "    scores = [d_scores[k][modelname] for k in d_scores.keys()]\n",
    "    M = max(scores)\n",
    "    m = min(scores)\n",
    "    \n",
    "    # Normalize the scores\n",
    "    # If there is only one word, thus one score in the generated vocabulary\n",
    "    if m == M:\n",
    "        d_normalized = {k: {modelname: d_scores[k][modelname] / M} for k in d_scores.keys()}\n",
    "    else:\n",
    "        d_normalized = {k: {modelname: (d_scores[k][modelname] - m) / (M - m)} for k in d_scores.keys()}\n",
    "    \n",
    "    return d_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all the result obtained :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mixing_model(models, word, n, generator = VocabularyGenerator()):\n",
    "    \"\"\"\n",
    "    Function that takes as input a list of models and a word and outputs\n",
    "    a normalized dictionary that gathers all the scores of the different\n",
    "    models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models    : list of models (names).\n",
    "    word      : the word in question.\n",
    "    n         : input to the generators (number of words).\n",
    "    generator : default value, our created classe.\n",
    "    \n",
    "    Returns\n",
    "    ------- \n",
    "    d_score, d_count : output dictionnary with the generate words \n",
    "                       (scores and counts).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    for modelname in models :\n",
    "        d_scores = getattr(generator, \"generate_words_{}\".format(modelname))(word, n)\n",
    "        d.update(normalize_scores(d_scores))\n",
    "        \n",
    "    d_count = {}\n",
    "    d_score = {}\n",
    "    # counting occurrence (could also use counter)\n",
    "    for key in d.keys() :\n",
    "        d_count[key] = len(d[key])\n",
    "        d_score[key] = sum(d[key].values())\n",
    "        \n",
    "    return d_score, d_count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\n",
      "100%|██████████| 4/4 [05:27<00:00, 93.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'aliment': 0.6576410015163425,\n",
       "  'back': 0.08742679892319248,\n",
       "  'backup': 0.08742679892319248,\n",
       "  'baffl': 0.025090256185177198,\n",
       "  'bananas': 0.2628202745200088,\n",
       "  'beat': 0.05388076962859285,\n",
       "  'beef': 0.3884821873510358,\n",
       "  'begin': 0.025090256185177198,\n",
       "  'beverages': 0.4727345793662483,\n",
       "  'brook': 0.08742679892319248,\n",
       "  'canned': 0.3201439373655058,\n",
       "  'cargo_deck': 0.0,\n",
       "  'cargo_hold': 0.0,\n",
       "  'cashew': 0.34221075709228455,\n",
       "  'chemicals': 0.15260005336242072,\n",
       "  'citrus': 0.10778973700420358,\n",
       "  'cocoa': 0.2415689492826425,\n",
       "  'come': 0.17107332539762463,\n",
       "  'comest': 0.6576410015163425,\n",
       "  'conserv': 0.2730867346391723,\n",
       "  'cooking': 0.21667354480349313,\n",
       "  'curb': 0.0,\n",
       "  'dairy': 0.5722277304830711,\n",
       "  'donjon': 0.0,\n",
       "  'draw': 0.025090256185177198,\n",
       "  'dungeon': 0.0,\n",
       "  'eatabl': 0.6576410015163425,\n",
       "  'edibl': 0.6576410015163425,\n",
       "  'fermented': 0.019844326666481577,\n",
       "  'fertilizers': 0.15009239604327532,\n",
       "  'fish': 0.15194925770237605,\n",
       "  'food': 1.0,\n",
       "  'foods': 1.0,\n",
       "  'foodstuffs': 0.2618819617606964,\n",
       "  'fruit': 0.12045131243791442,\n",
       "  'fruits': 0.32055239046233786,\n",
       "  'grain': 0.03653302376227653,\n",
       "  'gravel': 0.17107332539762463,\n",
       "  'guard': 0.05388076962859285,\n",
       "  'handgrip': 0.08742679892319248,\n",
       "  'handl': 0.08742679892319248,\n",
       "  'harbor': 0.025090256185177198,\n",
       "  'harbour': 0.025090256185177198,\n",
       "  'have': 0.12726317798490064,\n",
       "  'ingredients': 0.007667436359195912,\n",
       "  'liquor': 0.12154961834575555,\n",
       "  'livestock': 0.5626535522478802,\n",
       "  'maize': 0.26854614696500057,\n",
       "  'meat': 0.8698827485505327,\n",
       "  'milk': 0.20199323228981725,\n",
       "  'moder': 0.08742679892319248,\n",
       "  'nurs': 0.025090256185177198,\n",
       "  'nutrient': 1.0,\n",
       "  'nutriment': 0.6576410015163425,\n",
       "  'nutrit': 0.12726317798490064,\n",
       "  'nutrition': 0.06341375498743348,\n",
       "  'pabulum': 0.6576410015163425,\n",
       "  'pork': 0.031143136929195032,\n",
       "  'potatoes': 0.19547586432769232,\n",
       "  'poultry': 0.4434068941047701,\n",
       "  'produc': 0.2199835256540523,\n",
       "  'products': 0.596951377611594,\n",
       "  'rice': 0.21567264648902093,\n",
       "  'rich_person': 0.12726317798490064,\n",
       "  'seafood': 0.2676845368033651,\n",
       "  'shrimp': 0.11150581316282455,\n",
       "  'solid_food': 0.2804416891778304,\n",
       "  'sorghum': 0.17328952030760095,\n",
       "  'soybeans': 0.00782037098646599,\n",
       "  'stick': 0.025090256185177198,\n",
       "  'stomach': 0.025090256185177198,\n",
       "  'sugar': 0.0,\n",
       "  'sugarcane': 0.1437298449807561,\n",
       "  'susten': 0.6576410015163425,\n",
       "  'textiles': 0.33187755253775014,\n",
       "  'tobacco': 0.7138687237393598,\n",
       "  'vegetable': 0.39103501920623635,\n",
       "  'vegetables': 0.47341878536025045,\n",
       "  'victual': 0.6576410015163425,\n",
       "  'wealthy_person': 0.12726317798490064},\n",
       " {'aliment': 1,\n",
       "  'back': 1,\n",
       "  'backup': 1,\n",
       "  'baffl': 1,\n",
       "  'bananas': 1,\n",
       "  'beat': 1,\n",
       "  'beef': 1,\n",
       "  'begin': 1,\n",
       "  'beverages': 1,\n",
       "  'brook': 1,\n",
       "  'canned': 1,\n",
       "  'cargo_deck': 1,\n",
       "  'cargo_hold': 1,\n",
       "  'cashew': 1,\n",
       "  'chemicals': 1,\n",
       "  'citrus': 1,\n",
       "  'cocoa': 1,\n",
       "  'come': 1,\n",
       "  'comest': 1,\n",
       "  'conserv': 1,\n",
       "  'cooking': 1,\n",
       "  'curb': 1,\n",
       "  'dairy': 1,\n",
       "  'donjon': 1,\n",
       "  'draw': 1,\n",
       "  'dungeon': 1,\n",
       "  'eatabl': 1,\n",
       "  'edibl': 1,\n",
       "  'fermented': 1,\n",
       "  'fertilizers': 1,\n",
       "  'fish': 1,\n",
       "  'food': 1,\n",
       "  'foods': 1,\n",
       "  'foodstuffs': 1,\n",
       "  'fruit': 1,\n",
       "  'fruits': 1,\n",
       "  'grain': 1,\n",
       "  'gravel': 1,\n",
       "  'guard': 1,\n",
       "  'handgrip': 1,\n",
       "  'handl': 1,\n",
       "  'harbor': 1,\n",
       "  'harbour': 1,\n",
       "  'have': 1,\n",
       "  'ingredients': 1,\n",
       "  'liquor': 1,\n",
       "  'livestock': 1,\n",
       "  'maize': 1,\n",
       "  'meat': 1,\n",
       "  'milk': 1,\n",
       "  'moder': 1,\n",
       "  'nurs': 1,\n",
       "  'nutrient': 1,\n",
       "  'nutriment': 1,\n",
       "  'nutrit': 1,\n",
       "  'nutrition': 1,\n",
       "  'pabulum': 1,\n",
       "  'pork': 1,\n",
       "  'potatoes': 1,\n",
       "  'poultry': 1,\n",
       "  'produc': 1,\n",
       "  'products': 1,\n",
       "  'rice': 1,\n",
       "  'rich_person': 1,\n",
       "  'seafood': 1,\n",
       "  'shrimp': 1,\n",
       "  'solid_food': 1,\n",
       "  'sorghum': 1,\n",
       "  'soybeans': 1,\n",
       "  'stick': 1,\n",
       "  'stomach': 1,\n",
       "  'sugar': 1,\n",
       "  'sugarcane': 1,\n",
       "  'susten': 1,\n",
       "  'textiles': 1,\n",
       "  'tobacco': 1,\n",
       "  'vegetable': 1,\n",
       "  'vegetables': 1,\n",
       "  'victual': 1,\n",
       "  'wealthy_person': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixing_model([\"word2vec\", \"wordnet\"], \"food\", n = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that generates the final output :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categories_generate(models, n, categories = CATEGORIES, d = ({}, {})) :\n",
    "    \"\"\"\n",
    "    Function that generate the final output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models     : list of models (names).\n",
    "    n          : input to the generators (number of words).\n",
    "    categories : list of categories (default value, the global variable\n",
    "                 CATEGORIES).\n",
    "    dics        : already generated dictionaries to complete \n",
    "                 (d_output_score, d_output_count)\n",
    "                 \n",
    "    Returns\n",
    "    -------\n",
    "    d_output_score, d_output_count : output dictionnary with the generate words \n",
    "                       (scores and counts) for each category.\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    d_output_score = d[0]\n",
    "    d_output_count = d[1]\n",
    "    for word in tqdm(categories):\n",
    "        if word in d_output_score.keys:\n",
    "            continue\n",
    "        print(\">>> Generating vocabulary for {}\".format(word))\n",
    "        d_output_score[word], d_output_count[word] = mixing_model(models, word, n)\n",
    "        # Select only the top n similar words\n",
    "        d_output_score[word] = {k: d_output_score[word][k] for k in sorted(d_output_score[word], \n",
    "                                key = lambda k: d_output_score[word][k], reverse = True)[:n]}\n",
    "        d_output_count[word] = {k: d_output_count[word][k] for k in d_output_score[word].keys()}\n",
    "\n",
    "        \n",
    "    return d_output_score, d_output_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating vocabulary for advice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.83s/it]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  8%|▊         | 1/12 [00:12<02:21, 12.89s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating vocabulary for hygiene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 17%|█▋        | 2/12 [00:14<01:35,  9.59s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating vocabulary for equipment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:02<00:06,  2.17s/it]\n",
      " 50%|█████     | 2/4 [00:35<00:22, 11.42s/it]\n",
      " 75%|███████▌  | 3/4 [07:01<02:04, 124.04s/it]\n",
      "100%|██████████| 4/4 [56:27<00:00, 976.49s/it]\n",
      " 25%|██▌       | 3/12 [56:44<2:33:31, 1023.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating vocabulary for activities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 33%|███▎      | 4/12 [56:45<1:35:34, 716.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating vocabulary for technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 42%|████▏     | 5/12 [56:47<58:36, 502.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating vocabulary for info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:02<00:06,  2.10s/it]\n",
      " 50%|█████     | 2/4 [00:04<00:04,  2.07s/it]\n",
      " 75%|███████▌  | 3/4 [00:06<00:02,  2.06s/it]\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.05s/it]\n",
      " 50%|█████     | 6/12 [56:58<35:29, 354.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating vocabulary for administrative\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'administrative'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3dba10211d43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategories_generate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word2vec\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wordnet\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-17b3a6fcbea5>\u001b[0m in \u001b[0;36mcategories_generate\u001b[1;34m(models, n, categories)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">>> Generating vocabulary for {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0md_output_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_output_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixing_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Select only the top n similar words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         d_output_score[word] = {k: d_output_score[word][k] for k in sorted(d_output_score[word], \n",
      "\u001b[1;32m<ipython-input-8-395853951268>\u001b[0m in \u001b[0;36mmixing_model\u001b[1;34m(models, word, n, generator)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodelname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0md_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"generate_words_{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ed460dc2aab4>\u001b[0m in \u001b[0;36mgenerate_words_wordnet\u001b[1;34m(self, word, n, depth, synsets_indices)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Only relevant synsets of word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCATEGORIES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0msynsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ed460dc2aab4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Only relevant synsets of word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCATEGORIES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0msynsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'administrative'"
     ]
    }
   ],
   "source": [
    "d = categories_generate([\"word2vec\", \"wordnet\"], 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps <a id=\"next\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next steps for the project would be : **\n",
    "<ul>\n",
    "<li> Generate the lemmas from the stems for the output. \n",
    "<li> Creating other models *(spacy, word2vec with other training set, mixing word2vec with wordnet with scores as the number of occurence of each word, learn a model from the users (for instance if given a same keyword from different questions, the user answer with food related ressources 2 times in a row, then we may attribute this keyword to food and with a score of 2)* ....)\n",
    "<li> Defining a relevant threshold in order to extract the most relevant words.\n",
    "<li> Find a way to combine smartly the output of the different models for instance if model 1 generates fruit and model 2 generates fruit, it should be counted as 1 word with 2 occurences.\n",
    "<li> Finally, may be we will need manual data cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Documentation (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import error : https://stackoverflow.com/questions/15526996/ipython-notebook-locale-error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
