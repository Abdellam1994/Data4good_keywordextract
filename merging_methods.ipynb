{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1><center style=\"color:#2B3698\">Merging the different methods for generating matching keywords</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing the different libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code that was previously written by the appmachine"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stemming.porter2 import stem\n",
    "from itertools import repeat\n",
    "\n",
    "model       = gensim.models.KeyedVectors.load_word2vec_format('/home/theapemachine/data/word2vec/text8-vector.bin', binary=True)\n",
    "nlp         = spacy.load('en')\n",
    "w2v_words   = []\n",
    "wn_words    = []\n",
    "spacy_words = []\n",
    "w2v_stems   = []\n",
    "wn_stems    = []\n",
    "spacy_stems = []\n",
    "words       = []\n",
    "categories  = [\n",
    "    'advice',\n",
    "    'hygiene',\n",
    "    'equipment',\n",
    "    'activities',\n",
    "    'technology',\n",
    "    'info',\n",
    "    'administrative',\n",
    "    'job',\n",
    "    'education',\n",
    "    'home',\n",
    "    'health',\n",
    "    'food'\n",
    "]\n",
    "\n",
    "def most_similar(word):\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:10]\n",
    "\n",
    "def iterate(origin):\n",
    "    try:\n",
    "        for synset in wn.synsets(origin):\n",
    "            for lemma in synset.lemma_names():\n",
    "                if lemma.find('_') == -1:\n",
    "                    w2v_words.append(lemma)\n",
    "                    w2v_stems.append(stem(lemma))\n",
    "\n",
    "        for word in model.most_similar(origin):\n",
    "            if word[0].find('_') == -1:\n",
    "                wn_words.append(word[0])\n",
    "                wn_stems.append(stem(word[0]))\n",
    "\n",
    "        for w in most_similar(nlp.vocab[u''.join([origin])]):\n",
    "            spacy_words.append(w.lower_)\n",
    "            spacy_stems.append(stem(w.lower_))\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def clean(w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words):\n",
    "    for w in w2v_words:\n",
    "        if stem(w) in wn_stems or stem(w) in spacy_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    for w in wn_words:\n",
    "        if stem(w) in w2v_stems or stem(w) in spacy_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    for w in spacy_words:\n",
    "        if stem(w) in w2v_stems or stem(w) in wn_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    return [], [], [], [], [], []\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    iterate(sys.argv[1])\n",
    "\n",
    "    w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "    )\n",
    "\n",
    "    for i in repeat(None, 2):\n",
    "        for w in np.unique(words):\n",
    "            iterate(w)\n",
    "\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "        )\n",
    "\n",
    "    for w in np.unique(words):\n",
    "        print w\n",
    "\n",
    "    words = []\n",
    "else:\n",
    "    for category in categories:\n",
    "        print \"------------------\"\n",
    "        print category\n",
    "        print \"------------------\"\n",
    "        iterate(category)\n",
    "\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "        )\n",
    "\n",
    "        for i in repeat(None, 2):\n",
    "            for w in np.unique(words):\n",
    "                iterate(w)\n",
    "\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "                w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "            )\n",
    "\n",
    "        for w in np.unique(words):\n",
    "            print w\n",
    "\n",
    "        print\n",
    "        words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Merging the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "categories  = [\n",
    "    'advice',\n",
    "    'hygiene',\n",
    "    'equipment',\n",
    "    'activities',\n",
    "    'technology',\n",
    "    'info',\n",
    "    'administrative',\n",
    "    'job',\n",
    "    'education',\n",
    "    'home',\n",
    "    'health',\n",
    "    'food'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hypothesis : our input will be supposed to be a dictionnary of the form {\"name_category\" : {\"matching_keywords\" : {\"Model\" : score}}} and the return would be {\"name_category\" : {\"matching_keywords\" : score}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Question : Deal with stems or all words ?\n",
    "Hypothesis : we will keep only stems in our inputs and then generate the lems afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TO DO : create a function generate_input(models, name_models, category) and return an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from stemming.porter2 import stem\n",
    "from nltk.corpus import wordnet_ic\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def score_wordnet(word, matching_keyword) : \n",
    "    \"\"\"\n",
    "    Function that generates a score for the wordnet outputs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word              : string we want to compute the similarity to.\n",
    "    matching_keywords : string for computring the similarity.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score : The similarity score (float).\n",
    "    \"\"\"\n",
    "    \n",
    "    word = wn.synsets(word)[0]\n",
    "    matching_keyword = wn.synsets(matching_keyword)[0]\n",
    "    score = 0\n",
    "    try :\n",
    "        score += word.path_similarity(matching_keyword)\n",
    "        score += word.lch_similarity(matching_keyword)\n",
    "        score += word.wup_similarity(matching_keyword)\n",
    "    except :\n",
    "        pass\n",
    "    # TO DO add an other argument to the function (needs 3 arguments)\n",
    "    # score += word.jcn_similarity(matching_keyword) \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_words_wordnet(word, n = 3) :\n",
    "    \"\"\"\n",
    "    Function that generates matching keywords given a word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word : word to compute similarities to (can be a category word).\n",
    "    n    : number of layers we use when generating maching keywords.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d : {word: {similar_word: {word2vec: word2vec_score}}\n",
    "    \"\"\"\n",
    "    \n",
    "    name_model = \"wordnet\"\n",
    "    d = {}\n",
    "    \n",
    "    # First iteration\n",
    "    \n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemma_names():\n",
    "            d[stem(lemma)] = {name_model : score_wordnet(word, lemma)}\n",
    "    \n",
    "    # Other iterations\n",
    "    \n",
    "    for i in range(n) :\n",
    "        for origin in d.keys():\n",
    "            for synset in wn.synsets(origin):\n",
    "                for lemma in synset.lemma_names():\n",
    "                    d[stem(lemma)] = {name_model : score_wordnet(word, lemma)}\n",
    "          \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules of thumbs to know the correspondance between the number of words and n (number of layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:10<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 5, 2: 10, 3: 16, 4: 34, 5: 253, 6: 1393, 7: 5447}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corresp = {}\n",
    "\n",
    "for n in tqdm(range(7)) :\n",
    "    corresp[n + 1] = len(generate_words_wordnet(\"food\", n = n))\n",
    "    \n",
    "print(corresp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nasser benab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_words_word2vec(word, n, path, model_name = \"text8-vector.bin\"):\n",
    "    \"\"\" \n",
    "    Most similar words to word and their scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word: word to compute similarities to (can be a category word)\n",
    "    n: number of similar words to get\n",
    "    path: path of the pretrained word2vec model\n",
    "    model_name: name of the pretrained word2vec model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    d: {word: {similar_word: {word2vec: word2vec_score}}\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    # Load Google's pre-trained Word2Vec model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(path, \"text8-vector.bin\"), binary=True) \n",
    "    d[word] = {key: {\"word2vec\": value} for (key, value) in model.most_similar(word, topn = n)}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food': {'bananas': {'word2vec': 0.6068102717399597},\n",
       "  'beef': {'word2vec': 0.6227272748947144},\n",
       "  'beverages': {'word2vec': 0.6333991289138794},\n",
       "  'canned': {'word2vec': 0.6140711903572083},\n",
       "  'cashew': {'word2vec': 0.6168662905693054},\n",
       "  'chemicals': {'word2vec': 0.5928491950035095},\n",
       "  'citrus': {'word2vec': 0.5871732831001282},\n",
       "  'cocoa': {'word2vec': 0.6041184663772583},\n",
       "  'cooking': {'word2vec': 0.6009650826454163},\n",
       "  'dairy': {'word2vec': 0.6460014581680298},\n",
       "  'fermented': {'word2vec': 0.5760336518287659},\n",
       "  'fertilizers': {'word2vec': 0.5925315618515015},\n",
       "  'fish': {'word2vec': 0.5927667617797852},\n",
       "  'foods': {'word2vec': 0.7001853585243225},\n",
       "  'foodstuffs': {'word2vec': 0.6066914200782776},\n",
       "  'fruit': {'word2vec': 0.5887770652770996},\n",
       "  'fruits': {'word2vec': 0.6141229271888733},\n",
       "  'grain': {'word2vec': 0.5781475305557251},\n",
       "  'ingredients': {'word2vec': 0.5744912624359131},\n",
       "  'liquor': {'word2vec': 0.5889161825180054},\n",
       "  'livestock': {'word2vec': 0.6447887420654297},\n",
       "  'maize': {'word2vec': 0.6075355410575867},\n",
       "  'meat': {'word2vec': 0.6837040185928345},\n",
       "  'milk': {'word2vec': 0.5991055965423584},\n",
       "  'nutrition': {'word2vec': 0.5815523862838745},\n",
       "  'pork': {'word2vec': 0.5774648189544678},\n",
       "  'potatoes': {'word2vec': 0.5982800722122192},\n",
       "  'poultry': {'word2vec': 0.629684329032898},\n",
       "  'products': {'word2vec': 0.6491330862045288},\n",
       "  'rice': {'word2vec': 0.600838303565979},\n",
       "  'seafood': {'word2vec': 0.6074264049530029},\n",
       "  'shrimp': {'word2vec': 0.5876439809799194},\n",
       "  'sorghum': {'word2vec': 0.5954698324203491},\n",
       "  'soybeans': {'word2vec': 0.5745106339454651},\n",
       "  'sugar': {'word2vec': 0.5735200643539429},\n",
       "  'sugarcane': {'word2vec': 0.5917256474494934},\n",
       "  'textiles': {'word2vec': 0.6155574321746826},\n",
       "  'tobacco': {'word2vec': 0.6639424562454224},\n",
       "  'vegetable': {'word2vec': 0.6230506300926208},\n",
       "  'vegetables': {'word2vec': 0.6334857940673828}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_words_word2vec(\"food\", 40, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO : generate a function that outputs scores and list of matching keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a function that normalizes the scores of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of a more simple way to do it (may be when generating the scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_scores(d_scores):\n",
    "    \"\"\"\"\n",
    "    The function normalize the scores (between 0 and 1).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_scores : dictionnary of the form {matching_keywords : {model : score}}.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d_normalized : same dictionnary with normalized scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting the model used (hypothesis : same models used for all categories)\n",
    "    models = d_scores.values()[0].keys()\n",
    "    \n",
    "    # We keep the maximum and minimum for each model\n",
    "    max_dic = {}\n",
    "    min_dic = {}\n",
    "    \n",
    "    for model in models :\n",
    "        temp = [dic[model] for dic in d_scores.values()]\n",
    "        max_dic[model] = max(temp)\n",
    "        min_dic[model] = min(temp)\n",
    "        \n",
    "    d_normalized = {}\n",
    "    \n",
    "    # may be an easier way to do it with comprehensive dictionaries\n",
    "    for key in d_scores.keys() :\n",
    "        print(d_scores[key])\n",
    "        d_normalized[key] = {key_bis : (float(value - min_dic[key_bis]) / (max_dic[key_bis] - min_dic[key_bis])) \\\n",
    "                             for (key_bis, value) in d_scores[key].iteritems()}\n",
    "    \n",
    "    return d_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO : finish the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mixing_model():\n",
    "    for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "http://ftp.cs.toronto.edu/pub/gh/Budanitsky+Hirst-2001.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Documentation (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "import error : https://stackoverflow.com/questions/15526996/ipython-notebook-locale-error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
