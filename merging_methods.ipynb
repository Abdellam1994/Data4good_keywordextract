{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center style=\"color:#2B3698\">Merging the different methods for generating matching keywords</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the different libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code that was previously written by the appmachine"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from stemming.porter2 import stem\n",
    "from itertools import repeat\n",
    "\n",
    "model       = gensim.models.KeyedVectors.load_word2vec_format('/home/theapemachine/data/word2vec/text8-vector.bin', binary=True)\n",
    "nlp         = spacy.load('en')\n",
    "w2v_words   = []\n",
    "wn_words    = []\n",
    "spacy_words = []\n",
    "w2v_stems   = []\n",
    "wn_stems    = []\n",
    "spacy_stems = []\n",
    "words       = []\n",
    "categories  = [\n",
    "    'advice',\n",
    "    'hygiene',\n",
    "    'equipment',\n",
    "    'activities',\n",
    "    'technology',\n",
    "    'info',\n",
    "    'administrative',\n",
    "    'job',\n",
    "    'education',\n",
    "    'home',\n",
    "    'health',\n",
    "    'food'\n",
    "]\n",
    "\n",
    "def most_similar(word):\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:10]\n",
    "\n",
    "def iterate(origin):\n",
    "    try:\n",
    "        for synset in wn.synsets(origin):\n",
    "            for lemma in synset.lemma_names():\n",
    "                if lemma.find('_') == -1:\n",
    "                    w2v_words.append(lemma)\n",
    "                    w2v_stems.append(stem(lemma))\n",
    "\n",
    "        for word in model.most_similar(origin):\n",
    "            if word[0].find('_') == -1:\n",
    "                wn_words.append(word[0])\n",
    "                wn_stems.append(stem(word[0]))\n",
    "\n",
    "        for w in most_similar(nlp.vocab[u''.join([origin])]):\n",
    "            spacy_words.append(w.lower_)\n",
    "            spacy_stems.append(stem(w.lower_))\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def clean(w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words):\n",
    "    for w in w2v_words:\n",
    "        if stem(w) in wn_stems or stem(w) in spacy_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    for w in wn_words:\n",
    "        if stem(w) in w2v_stems or stem(w) in spacy_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    for w in spacy_words:\n",
    "        if stem(w) in w2v_stems or stem(w) in wn_stems:\n",
    "            words.append(w)\n",
    "\n",
    "    return [], [], [], [], [], []\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    iterate(sys.argv[1])\n",
    "\n",
    "    w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "    )\n",
    "\n",
    "    for i in repeat(None, 2):\n",
    "        for w in np.unique(words):\n",
    "            iterate(w)\n",
    "\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "        )\n",
    "\n",
    "    for w in np.unique(words):\n",
    "        print w\n",
    "\n",
    "    words = []\n",
    "else:\n",
    "    for category in categories:\n",
    "        print \"------------------\"\n",
    "        print category\n",
    "        print \"------------------\"\n",
    "        iterate(category)\n",
    "\n",
    "        w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "        )\n",
    "\n",
    "        for i in repeat(None, 2):\n",
    "            for w in np.unique(words):\n",
    "                iterate(w)\n",
    "\n",
    "            w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems = clean(\n",
    "                w2v_words, wn_words, spacy_words, w2v_stems, wn_stems, spacy_stems, words\n",
    "            )\n",
    "\n",
    "        for w in np.unique(words):\n",
    "            print w\n",
    "\n",
    "        print\n",
    "        words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from stemming.porter2 import stem\n",
    "from nltk.corpus import wordnet_ic\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories  = [\n",
    "    'advice',\n",
    "    'hygiene',\n",
    "    'equipment',\n",
    "    'activities',\n",
    "    'technology',\n",
    "    'info',\n",
    "    'administrative',\n",
    "    'job',\n",
    "    'education',\n",
    "    'home',\n",
    "    'health',\n",
    "    'food'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the right synset for each category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> advice\n",
      "0 - ['advice'] : a proposal for an appropriate course of action\n",
      "\n",
      "\n",
      ">>> hygiene\n",
      "0 - ['hygiene'] : a condition promoting sanitary practices\n",
      "1 - ['hygiene', 'hygienics'] : the science concerned with the prevention of illness and maintenance of health\n",
      "\n",
      "\n",
      ">>> equipment\n",
      "0 - ['equipment'] : an instrumentality needed for an undertaking or to perform a service\n",
      "\n",
      "\n",
      ">>> activities\n",
      "0 - ['activity'] : any specific behavior\n",
      "1 - ['action', 'activity', 'activeness'] : the state of being active\n",
      "2 - ['bodily_process', 'body_process', 'bodily_function', 'activity'] : an organic process that takes place in the body\n",
      "3 - ['activity'] : (chemistry) the capacity of a substance to take part in a chemical reaction\n",
      "4 - ['natural_process', 'natural_action', 'action', 'activity'] : a process existing in or produced by nature (rather than by the intent of human beings)\n",
      "5 - ['activeness', 'activity'] : the trait of being active; moving or acting rapidly and energetically\n",
      "\n",
      "\n",
      ">>> technology\n",
      "0 - ['technology', 'engineering'] : the practical application of science to commerce or industry\n",
      "1 - ['engineering', 'engineering_science', 'applied_science', 'technology'] : the discipline dealing with the art or science of applying scientific knowledge to practical problems\n",
      "\n",
      "\n",
      ">>> info\n",
      "0 - ['information', 'info'] : a message received and understood\n",
      "\n",
      "\n",
      ">>> administrative\n",
      "0 - ['administrative'] : of or relating to or responsible for administration\n",
      "\n",
      "\n",
      ">>> job\n",
      "0 - ['occupation', 'business', 'job', 'line_of_work', 'line'] : the principal activity in your life that you do to earn money\n",
      "1 - ['job', 'task', 'chore'] : a specific piece of work required to be done as a duty or for a specific fee\n",
      "2 - ['job'] : a workplace; as in the expression \"on the job\"; \n",
      "3 - ['job'] : an object worked on; a result produced by working\n",
      "4 - ['job'] : the responsibility to do something\n",
      "5 - ['job'] : the performance of a piece of work\n",
      "6 - ['job'] : a damaging piece of work\n",
      "7 - ['problem', 'job'] : a state of difficulty that needs to be resolved\n",
      "8 - ['Job'] : a Jewish hero in the Old Testament who maintained his faith in God in spite of afflictions that tested him\n",
      "9 - ['Job'] : any long-suffering person who withstands affliction without despairing\n",
      "10 - ['job'] : (computer science) a program application that may consist of several steps but is a single logical unit\n",
      "11 - ['Job', 'Book_of_Job'] : a book in the Old Testament containing Job's pleas to God about his afflictions and God's reply\n",
      "12 - ['caper', 'job'] : a crime (especially a robbery)\n",
      "13 - ['job'] : profit privately from public office and official business\n",
      "14 - ['subcontract', 'farm_out', 'job'] : arranged for contracted work to be done by others\n",
      "15 - ['job'] : work occasionally\n",
      "16 - ['speculate', 'job'] : invest at a risk\n",
      "\n",
      "\n",
      ">>> education\n",
      "0 - ['education', 'instruction', 'teaching', 'pedagogy', 'didactics', 'educational_activity'] : the activities of educating or instructing; activities that impart knowledge or skill\n",
      "1 - ['education'] : knowledge acquired by learning and instruction\n",
      "2 - ['education'] : the gradual process of acquiring knowledge\n",
      "3 - ['education'] : the profession of teaching (especially at a school or college or university)\n",
      "4 - ['education', 'training', 'breeding'] : the result of good upbringing (especially knowledge of correct social behavior)\n",
      "5 - ['Department_of_Education', 'Education_Department', 'Education'] : the United States federal department that administers all federal programs dealing with education (including federal aid to educational institutions and students); created 1979\n",
      "\n",
      "\n",
      ">>> home\n",
      "0 - ['home', 'place'] : where you live at a particular time\n",
      "1 - ['dwelling', 'home', 'domicile', 'abode', 'habitation', 'dwelling_house'] : housing that someone is living in\n",
      "2 - ['home'] : the country or state or city where you live\n",
      "3 - ['home_plate', 'home_base', 'home', 'plate'] : (baseball) base consisting of a rubber slab where the batter stands; it must be touched by a base runner in order to score\n",
      "4 - ['base', 'home'] : the place where you are stationed and from which missions start and end\n",
      "5 - ['home'] : place where something began and flourished\n",
      "6 - ['home'] : an environment offering affection and security\n",
      "7 - ['family', 'household', 'house', 'home', 'menage'] : a social unit living together\n",
      "8 - ['home', 'nursing_home', 'rest_home'] : an institution where people are cared for\n",
      "9 - ['home'] : provide with, or send to, a home\n",
      "10 - ['home'] : return home accurately from a long distance\n",
      "11 - ['home'] : used of your own ground\n",
      "12 - ['home'] : relating to or being where one lives or where one's roots are\n",
      "13 - ['home', 'interior', 'internal', 'national'] : inside the country\n",
      "14 - ['home'] : at or to or in the direction of one's home or family\n",
      "15 - ['home'] : on or to the point aimed at\n",
      "16 - ['home'] : to the fullest extent; to the heart\n",
      "\n",
      "\n",
      ">>> health\n",
      "0 - ['health', 'wellness'] : a healthy state of wellbeing free from disease\n",
      "1 - ['health'] : the general condition of body and mind\n",
      "\n",
      "\n",
      ">>> food\n",
      "0 - ['food', 'nutrient'] : any substance that can be metabolized by an animal to give energy and build tissue\n",
      "1 - ['food', 'solid_food'] : any solid substance (as opposed to liquid) that is used as a source of nourishment\n",
      "2 - ['food', 'food_for_thought', 'intellectual_nourishment'] : anything that provides mental stimulus for thinking\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each category, for each synset print its lemma names and the definition\n",
    "for category in categories:\n",
    "    print(\">>> {}\".format(category))\n",
    "    i = 0\n",
    "    for synset in wn.synsets(category):\n",
    "        print(i, \"-\", synset.lemma_names(), \":\", synset.definition())\n",
    "        i += 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each category, we will select only the relevant synsets\n",
    "d_category_synsets = {\"advice\": [0], \"hygiene\": [0], \"equipment\": [0], \"activities\": [0], \"technology\": [0], \"info\": [0], \n",
    "                      \"job\": [0, 1], \"education\": [0], \"home\": [0], \"health\": [0], \"food\": [0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis : our input will be supposed to be a dictionnary of the form {\"name_category\" : {\"matching_keywords\" : {\"Model\" : score}}} and the return would be {\"name_category\" : {\"matching_keywords\" : score}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question : Deal with stems or all words ?\n",
    "Hypothesis : we will keep only stems in our inputs and then generate the lems afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO : create a function generate_input(models, name_models, category) and return an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark : May be code models as classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute 4 different scores for wordnet (http://www.nltk.org/howto/wordnet.html):\n",
    "<li> path_similarity score :  Return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1.\n",
    "<li> lch_similarity : Return a score denoting how similar two word senses are, based on the shortest path that connects the senses (as above) and the maximum depth of the taxonomy in which the senses occur. The relationship is given as -log(p/2d) where p is the shortest path length and d the taxonomy depth. (Here is a website that simulates the path to better understand the taxonomy : http://ws4jdemo.appspot.com/?mode=w&s1=&w1=cat&s2=&w2=dog.\n",
    "<li> wup_similarity : The Wu & Palmer measure (wup) calculates similarity by considering the depths of the two concepts in wordnet, along with the depth of the LCS (least common ancestor in the taxonomy) The formula is score = 2*depth(lcs) / (depth(s1) + depth(s2)). This means that 0 $<$ score $<=$ 1. The score can never be zero because the depth of the LCS is never zero (the depth of the root of a taxonomy is one). The score is one if the two input concepts are the same.\n",
    "<li> jcn_similarity : Jiang-Conrath Similarity Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. The relationship is given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs)). (Information Content (IC) is a measure of specificity for a concept. Higher values are associated with more specific concepts (e.g., pitch fork), while those with lower values are more general (e.g., idea). In- formation Content is computed based on frequency counts of concepts as found in a corpus of text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_wordnet(word, matching_keyword) : \n",
    "    \"\"\"\n",
    "    Function that generates a score for the wordnet outputs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word              : string we want to compute the similarity to.\n",
    "    matching_keywords : string for computring the similarity.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score : The similarity score (float).\n",
    "    \"\"\"\n",
    "    \n",
    "    # We will nomalise the scores for each similarity\n",
    "    min_max_values = {'path' : [0, 1], 'lch' : [0, 3.6375861597263857 ], 'wup' : [0,1], 'jcn' : [0, 10000]}\n",
    "    # Reference for range : (lch) https://stackoverflow.com/questions/20112828/maximum-score-in-wordnet-based-similarity\n",
    "    # (jcn) https://stackoverflow.com/questions/35751207/how-to-normalize-similarity-measurements-lch-wup-path-res-lin-jcn-between\n",
    "    \n",
    "    # For jcn we will take 10 000 as the maximum (quite arbitrary but seemed relevant)\n",
    "    \n",
    "    word = wn.synsets(word)[0]\n",
    "    matching_keyword = wn.synsets(matching_keyword)[0]\n",
    "    score = 0\n",
    "    try :\n",
    "        score += word.path_similarity(matching_keyword)\n",
    "    except :\n",
    "        pass\n",
    "    try : \n",
    "        score += (word.lch_similarity(matching_keyword) /  min_max_values['lch'][1])\n",
    "    except :\n",
    "        pass\n",
    "    try :\n",
    "        score += word.wup_similarity(matching_keyword)\n",
    "    except :\n",
    "        pass\n",
    "    try : \n",
    "        brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "        score += (word.jcn_similarity(matching_keyword) / min_max_values['jcn'][1])\n",
    "    except :\n",
    "        pass\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does not render good results, may be we sould add a control variable that does control the score (for instance a synonym of the word or the real corresponding synset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_words_wordnet(word, n, depth = 3, synsets_indices = d_category_synsets) :\n",
    "    \"\"\"\n",
    "    Function that generates matching keywords given a word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word  : word to compute similarities to (can be a category word).\n",
    "    depth : number of layers we use when generating maching keywords.\n",
    "    n     : number of words we take.\n",
    "    synsets: dictionary mapping a word to the a list of indices\n",
    "             specifying its relevant synsets in wordnet\n",
    "             Ex: {\"food\": [0]}\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d : {word: {similar_word: {wordnet: wordnet_score}}\n",
    "    \"\"\"\n",
    "    \n",
    "    name_model = \"wordnet\"\n",
    "    d = {}\n",
    "    \n",
    "    # First iteration (select only relevant synsets as specified in d_category_synsets)\n",
    "    # All the synsets of word\n",
    "    synsets = wn.synsets(word)    \n",
    "    # Only relevant synsets of word\n",
    "    if word in categories:\n",
    "        synsets = [synsets[i] for i in range(len(synsets)) if i in synsets_indices[word]]\n",
    "    for synset in tqdm(synsets):\n",
    "        for lemma in synset.lemma_names():\n",
    "            d[stem(lemma)] = {name_model : score_wordnet(word, lemma)}\n",
    "    \n",
    "    # Other iterations\n",
    "    \n",
    "    for i in tqdm(range(depth)) :\n",
    "        dic = d.copy()\n",
    "        for origin in dic.keys():\n",
    "            for synset in wn.synsets(origin):\n",
    "                for lemma in synset.lemma_names():\n",
    "                    d[stem(lemma)] = {name_model : score_wordnet(word, lemma)}\n",
    "    \n",
    "    if n > len(d) :\n",
    "        return d\n",
    "    \n",
    "    d_max_values = {k: d[k] for k in sorted(d, key = lambda k: d[k][\"wordnet\"], reverse = True)[:n]}\n",
    "    \n",
    "    return d_max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.14s/it]\n",
      "100%|██████████| 3/3 [05:28<00:00, 96.80s/it]\n"
     ]
    }
   ],
   "source": [
    "d = generate_words_wordnet(\"education\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conduct': {'wordnet': 1.8884593473203033},\n",
       " 'didact': {'wordnet': 3.0},\n",
       " 'educ': {'wordnet': 3.0},\n",
       " 'educational_act': {'wordnet': 3.0},\n",
       " 'tri': {'wordnet': 1.8884593473203033}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_words_word2vec(word, n, path = path, model_name = \"text8-vector.bin\"):\n",
    "    \"\"\" \n",
    "    Most similar words to word and their scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word: word to compute similarities to (can be a category word)\n",
    "    n: number of similar words to get\n",
    "    path: path of the pretrained word2vec model\n",
    "    model_name: name of the pretrained word2vec model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    d: {word: {similar_word: {word2vec: word2vec_score}}\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    # Load Google's pre-trained Word2Vec model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(path, \"text8-vector.bin\"), binary=True) \n",
    "    d[word] = {key: {\"word2vec\": value} for (key, value) in model.most_similar(word, topn = n)}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "unknown URI scheme 'c' in 'C:\\\\Users\\\\Nasser Benab\\\\Documents\\\\git\\\\data/text8-vector.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5781f434b309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_words_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"food\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-de9c6bfbe358>\u001b[0m in \u001b[0;36mgenerate_words_word2vec\u001b[0;34m(word, n, path, model_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Load Google's pre-trained Word2Vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text8-vector.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"word2vec\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/smart_open/smart_open_lib.pyc\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# this method just routes the request to classes handling the specific storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# schemes, depending on the URI protocol in `uri`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mparsed_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParseUri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/smart_open/smart_open_lib.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, default_scheme)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown URI scheme %r in %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: unknown URI scheme 'c' in 'C:\\\\Users\\\\Nasser Benab\\\\Documents\\\\git\\\\data/text8-vector.bin'"
     ]
    }
   ],
   "source": [
    "generate_words_word2vec(\"food\", 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TO DO : generate a function that takes as input a word and returns matching keywords and scores using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO : generate a function that outputs scores and list of matching keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a function that normalizes the scores of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of a more simple way to do it (may be when generating the scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(d_scores):\n",
    "    \"\"\"\"\n",
    "    The function normalize the scores (between 0 and 1).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_scores : dictionnary of the form {matching_keywords : {model : score}}.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d_normalized : same dictionnary with normalized scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting the model used (hypothesis : same models used for all categories)\n",
    "    models = d_scores.values()[0].keys()\n",
    "    \n",
    "    # We keep the maximum and minimum for each model\n",
    "    max_dic = {}\n",
    "    min_dic = {}\n",
    "    \n",
    "    for model in models :\n",
    "        temp = [dic[model] for dic in d_scores.values()]\n",
    "        max_dic[model] = max(temp)\n",
    "        min_dic[model] = min(temp)\n",
    "        \n",
    "    d_normalized = {}\n",
    "    \n",
    "    # may be an easier way to do it with comprehensive dictionaries\n",
    "    for key in d_scores.keys() :\n",
    "        print(d_scores[key])\n",
    "        d_normalized[key] = {key_bis : (float(value - min_dic[key_bis]) / (max_dic[key_bis] - min_dic[key_bis])) \\\n",
    "                             for (key_bis, value) in d_scores[key].iteritems()}\n",
    "    \n",
    "    return d_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO : finish the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find a way to match the different outputs from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mixing_model(models, word, n):\n",
    "    # TO DO : Comment the function\n",
    "    # TO DO : Check the words output for each model and how to do \n",
    "    d = {}\n",
    "    for model in models :\n",
    "        d.update(normalize_score(model(word, n)))\n",
    "    \n",
    "    d_count = {}\n",
    "    d_score = {}\n",
    "    # counting occurrence (could also use counter)\n",
    "    for key in d.keys() :\n",
    "        d_count[key] = len(d[key])\n",
    "        d_score[key] = sum(d[key].values())\n",
    "        \n",
    "    return d_score, d_count    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO : generate lemas from obtained dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categories(list_categories, models, n) :\n",
    "    # Final function : TO DO : complete\n",
    "    d_output_score = {}\n",
    "    d_output_count = {}\n",
    "    for word in list_categories :\n",
    "        d_output_score[word], d_output_count[word] = mixing_model(models, word, n)\n",
    "        \n",
    "    return d_output_score, d_output_count\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ftp.cs.toronto.edu/pub/gh/Budanitsky+Hirst-2001.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import error : https://stackoverflow.com/questions/15526996/ipython-notebook-locale-error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
